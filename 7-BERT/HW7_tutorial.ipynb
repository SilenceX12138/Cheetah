{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Copy of hw7_tutorial",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.1 64-bit"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "e2e1f6a47c2b22aa38ff4f44d0d21f0cf81fc40f5c0e12b6a1c9dbd62fd843ec"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **A toy example for HW7 Bert QA**\n",
        "\n",
        "If you have any questions, feel free to email us at ntu-ml-2021spring-ta@googlegroups.com"
      ],
      "metadata": {
        "id": "xvSGDbExff_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install transformers\n",
        "Documentation for the toolkit:　https://huggingface.co/transformers/"
      ],
      "metadata": {
        "id": "64N6tucFQRl8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install transformers==4.5.0"
      ],
      "outputs": [],
      "metadata": {
        "id": "I7etZIyfmCVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ],
      "metadata": {
        "id": "tLSGMP5wQXOY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import torch\r\n",
        "from transformers import AdamW, BertTokenizerFast, BertForQuestionAnswering"
      ],
      "outputs": [],
      "metadata": {
        "id": "SdGJ3hFN_8Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model and Tokenizer\n",
        "A list of avaliable pretrained models: https://huggingface.co/models"
      ],
      "metadata": {
        "id": "11qNo2EPQhuN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# model_name can be either: models in huggingface model hub or models saved using save_pretrained\r\n",
        "model_name = 'bert-base-chinese'\r\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "metadata": {
        "id": "xODRE-8RmMoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "chi_tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\r\n",
        "eng_tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 213k/213k [00:08<00:00, 23.8kB/s]\n",
            "Downloading: 100%|██████████| 436k/436k [00:28<00:00, 15.1kB/s]\n",
            "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 29.2kB/s]\n"
          ]
        }
      ],
      "metadata": {
        "id": "ln-4faF8Q1h5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize"
      ],
      "metadata": {
        "id": "RQ5hBatuQ9Ix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "chi_paragraph = '李宏毅幾班大金。2021 ML'\r\n",
        "tokens = chi_tokenizer.tokenize(chi_paragraph)\r\n",
        "print(tokens)\r\n",
        "chi_tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['李', '宏', '毅', '幾', '班', '大', '金', '。', '2021', '[UNK]']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3330, 2131, 3675, 2407, 4408, 1920, 7032, 511, 9960, 100]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "metadata": {
        "id": "i5PRNQTwqZZd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "eng_paragraph = 'Lee Hung-yi which class Daikin.'\r\n",
        "tokens = eng_tokenizer.tokenize(eng_paragraph)\r\n",
        "print(tokens)\r\n",
        "eng_tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lee', 'Hung', '-', 'y', '##i', 'which', 'class', 'Dai', '##kin', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2499, 26157, 118, 194, 1182, 1134, 1705, 23084, 4314, 119]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "metadata": {
        "id": "EbtifQRwqouP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode vs Decode"
      ],
      "metadata": {
        "id": "xuq3PhS3RCdv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "question = '李宏毅幾班?'\r\n",
        "paragraph = '李宏毅幾班大金。'\r\n",
        "encoded = chi_tokenizer.encode(question, paragraph)\r\n",
        "decoded = chi_tokenizer.decode(encoded)\r\n",
        "print(encoded)\r\n",
        "print(decoded)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 3330, 2131, 3675, 2407, 4408, 136, 102, 3330, 2131, 3675, 2407, 4408, 1920, 7032, 511, 102]\n",
            "[CLS] 李 宏 毅 幾 班? [SEP] 李 宏 毅 幾 班 大 金 。 [SEP]\n"
          ]
        }
      ],
      "metadata": {
        "id": "lMX-gu57ssi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Inputs"
      ],
      "metadata": {
        "id": "4BzNV4smRJ2X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "inputs = chi_tokenizer(question, paragraph, return_tensors='pt') # set return type as PyTorch\r\n",
        "# Indices of input sequence tokens in the vocabulary\r\n",
        "print('Input ids:      ', inputs['input_ids'])\r\n",
        "# Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\r\n",
        "print('Token type ids: ', inputs['token_type_ids'])\r\n",
        "# Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\r\n",
        "print('Attention mask: ', inputs['attention_mask'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input ids:       tensor([[ 101, 3330, 2131, 3675, 2407, 4408,  136,  102, 3330, 2131, 3675, 2407,\n",
            "         4408, 1920, 7032,  511,  102]])\n",
            "Token type ids:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "Attention mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "XoITv6O0tCVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing (Chinese)"
      ],
      "metadata": {
        "id": "GFH-Fb6LVG-v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "question = '李宏毅幾班?'\r\n",
        "paragraph = '李宏毅幾班大金。'\r\n",
        "inputs = chi_tokenizer(question, paragraph, return_tensors='pt')\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    output = model(**inputs)\r\n",
        "# output = model(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\r\n",
        "\r\n",
        "print(\"start_logits: \")\r\n",
        "print(output.start_logits)\r\n",
        "\r\n",
        "print(\"end_logits: \")\r\n",
        "print(output.end_logits)\r\n",
        "\r\n",
        "start = torch.argmax(output.start_logits)\r\n",
        "end = torch.argmax(output.end_logits)\r\n",
        "print(\"start position: \", start.item())\r\n",
        "print(\"end position:   \", end.item())\r\n",
        "\r\n",
        "predict_id = inputs['input_ids'][0][start : end + 1]\r\n",
        "print(\"predict_id:     \", predict_id)\r\n",
        "\r\n",
        "predict_answer = chi_tokenizer.decode(predict_id)\r\n",
        "print(\"predict_answer: \", predict_answer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_logits: \n",
            "tensor([[-0.6262, -0.7415, -0.7111, -0.6126, -0.6379, -0.6177, -0.9293, -0.5159,\n",
            "         -0.7238, -0.6336, -0.4981, -0.5196, -0.4478, -0.7371, -1.0419, -0.5336,\n",
            "         -0.5159]])\n",
            "end_logits: \n",
            "tensor([[-0.0684,  0.7831,  0.3180,  0.5149,  0.6406, -0.1122,  0.7964, -0.2628,\n",
            "          0.1568,  0.3487,  0.5829,  0.6321, -0.0010,  0.0307,  0.2376,  0.4377,\n",
            "         -0.2628]])\n",
            "start position:  12\n",
            "end position:    6\n",
            "predict_id:      tensor([], dtype=torch.int64)\n",
            "predict_answer:  \n"
          ]
        }
      ],
      "metadata": {
        "id": "-sRQGEesjBmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training (Chinese)\n",
        "For Question Answering, loss is the sum of cross entropy between the model prediction and correct answer"
      ],
      "metadata": {
        "id": "fBGQCyCZUwaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "output = model(**inputs, start_positions=torch.tensor([13]), end_positions=torch.tensor([14]))\r\n",
        "print(\"loss: \", output.loss)\r\n",
        "\r\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\r\n",
        "output.loss.backward()\r\n",
        "optimizer.step()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  tensor(2.9323, grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "metadata": {
        "id": "dP3SRdS8wedu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing (English)"
      ],
      "metadata": {
        "id": "awNHFgkCyj9F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "question = \"Why does Jeanie like Tom?\"\r\n",
        "paragraph = \"Jeanie likes Tom because Tom is good at deep learning.\"\r\n",
        "inputs = eng_tokenizer(question, paragraph, return_tensors='pt')\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    output = model(**inputs)\r\n",
        "# output = model(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\r\n",
        "\r\n",
        "print(\"start_logits: \")\r\n",
        "print(output.start_logits)\r\n",
        "\r\n",
        "print(\"end_logits: \")\r\n",
        "print(output.end_logits)\r\n",
        "\r\n",
        "start = torch.argmax(output.start_logits)\r\n",
        "end = torch.argmax(output.end_logits)\r\n",
        "print(\"start position: \", start.item())\r\n",
        "print(\"end position:   \", end.item())\r\n",
        "\r\n",
        "predict_id = inputs['input_ids'][0][start : end + 1]\r\n",
        "print(\"predict_id:     \", predict_id)\r\n",
        "\r\n",
        "predict_answer = eng_tokenizer.decode(predict_id)\r\n",
        "print(\"predict_answer: \", predict_answer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_logits: \n",
            "tensor([[-0.8070, -0.4859, -0.9686, -0.7980, -0.8325, -0.4310, -0.2878, -0.9313,\n",
            "         -0.7036, -0.5790, -0.7574, -0.3716, -0.3959, -0.5793, -0.7177, -0.7736,\n",
            "         -0.6535, -0.4270, -0.2888, -0.3633, -0.8827, -0.7036]])\n",
            "end_logits: \n",
            "tensor([[-0.0328,  0.5448,  0.0546,  0.2758,  0.6124,  0.3178,  0.8662,  0.5064,\n",
            "          0.0151,  0.2450,  0.4484,  0.2418,  0.6935,  0.2410,  0.7174,  0.6489,\n",
            "          0.5701,  0.7711,  0.2106,  0.4943,  0.2235,  0.0151]])\n",
            "start position:  6\n",
            "end position:    6\n",
            "predict_id:      tensor([2545])\n",
            "predict_answer:  Tom\n"
          ]
        }
      ],
      "metadata": {
        "id": "gt0Glg0yyfg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training (English)\n",
        "For Question Answering, loss is the sum of cross entropy between the model prediction and correct answer"
      ],
      "metadata": {
        "id": "fQB51L0-yn9k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "output = model(**inputs, start_positions=torch.tensor([14]), end_positions=torch.tensor([19]))\r\n",
        "print(\"loss: \", output.loss)\r\n",
        "\r\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\r\n",
        "output.loss.backward()\r\n",
        "optimizer.step()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  tensor(3.1155, grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "metadata": {
        "id": "82AZ3K7e8ZjS"
      }
    }
  ]
}