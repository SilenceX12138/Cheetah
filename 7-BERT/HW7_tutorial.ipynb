{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Copy of hw7_tutorial",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.1 64-bit"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "e2e1f6a47c2b22aa38ff4f44d0d21f0cf81fc40f5c0e12b6a1c9dbd62fd843ec"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **A toy example for HW7 Bert QA**\n",
        "\n",
        "If you have any questions, feel free to email us at ntu-ml-2021spring-ta@googlegroups.com"
      ],
      "metadata": {
        "id": "xvSGDbExff_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install transformers\n",
        "Documentation for the toolkit:　https://huggingface.co/transformers/"
      ],
      "metadata": {
        "id": "64N6tucFQRl8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "!pip install transformers==4.5.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.doubanio.com/simple/\n",
            "Requirement already satisfied: transformers==4.5.0 in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.5.0)\n",
            "Requirement already satisfied: requests in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.5.0) (2.24.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.5.0) (4.56.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.5.0) (1.21.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.5.0) (20.4)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.5.0) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.5.0) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.5.0) (2021.7.6)\n",
            "Requirement already satisfied: filelock in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.5.0) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging->transformers==4.5.0) (2.4.7)\n",
            "Requirement already satisfied: six in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging->transformers==4.5.0) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers==4.5.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers==4.5.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers==4.5.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers==4.5.0) (1.25.8)\n",
            "Requirement already satisfied: joblib in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sacremoses->transformers==4.5.0) (0.17.0)\n",
            "Requirement already satisfied: click in c:\\users\\silence\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sacremoses->transformers==4.5.0) (7.1.2)\n"
          ]
        }
      ],
      "metadata": {
        "id": "I7etZIyfmCVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ],
      "metadata": {
        "id": "tLSGMP5wQXOY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import torch\r\n",
        "from transformers import AdamW, BertTokenizerFast, BertForQuestionAnswering"
      ],
      "outputs": [],
      "metadata": {
        "id": "SdGJ3hFN_8Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model and Tokenizer\n",
        "A list of avaliable pretrained models: https://huggingface.co/models"
      ],
      "metadata": {
        "id": "11qNo2EPQhuN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# model_name can be either: models in huggingface model hub or models saved using save_pretrained\r\n",
        "model_name = 'bert-base-chinese'\r\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "metadata": {
        "id": "xODRE-8RmMoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "chi_tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\r\n",
        "eng_tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
      ],
      "outputs": [],
      "metadata": {
        "id": "ln-4faF8Q1h5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize"
      ],
      "metadata": {
        "id": "RQ5hBatuQ9Ix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "chi_paragraph = '李宏毅幾班大金。2021 ML'\r\n",
        "tokens = chi_tokenizer.tokenize(chi_paragraph)\r\n",
        "print(tokens)\r\n",
        "chi_tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['李', '宏', '毅', '幾', '班', '大', '金', '。', '2021', '[UNK]']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3330, 2131, 3675, 2407, 4408, 1920, 7032, 511, 9960, 100]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "metadata": {
        "id": "i5PRNQTwqZZd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "eng_paragraph = 'Lee Hung-yi which class Daikin.'\r\n",
        "tokens = eng_tokenizer.tokenize(eng_paragraph)\r\n",
        "print(tokens)\r\n",
        "eng_tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lee', 'Hung', '-', 'y', '##i', 'which', 'class', 'Dai', '##kin', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2499, 26157, 118, 194, 1182, 1134, 1705, 23084, 4314, 119]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "metadata": {
        "id": "EbtifQRwqouP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode vs Decode"
      ],
      "metadata": {
        "id": "xuq3PhS3RCdv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "question = '李宏毅幾班?'\r\n",
        "paragraph = '李宏毅幾班大金。'\r\n",
        "encoded = chi_tokenizer.encode(question, paragraph)\r\n",
        "decoded = chi_tokenizer.decode(encoded)\r\n",
        "print(encoded)\r\n",
        "print(decoded)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 3330, 2131, 3675, 2407, 4408, 136, 102, 3330, 2131, 3675, 2407, 4408, 1920, 7032, 511, 102]\n",
            "[CLS] 李 宏 毅 幾 班? [SEP] 李 宏 毅 幾 班 大 金 。 [SEP]\n"
          ]
        }
      ],
      "metadata": {
        "id": "lMX-gu57ssi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Inputs"
      ],
      "metadata": {
        "id": "4BzNV4smRJ2X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "inputs = chi_tokenizer(question, paragraph, return_tensors='pt') # set return type as PyTorch\r\n",
        "# Indices of input sequence tokens in the vocabulary\r\n",
        "print('Input ids:      ', inputs['input_ids'])\r\n",
        "# Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\r\n",
        "print('Token type ids: ', inputs['token_type_ids'])\r\n",
        "# Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\r\n",
        "print('Attention mask: ', inputs['attention_mask'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input ids:       tensor([[ 101, 3330, 2131, 3675, 2407, 4408,  136,  102, 3330, 2131, 3675, 2407,\n",
            "         4408, 1920, 7032,  511,  102]])\n",
            "Token type ids:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "Attention mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "XoITv6O0tCVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing (Chinese)"
      ],
      "metadata": {
        "id": "GFH-Fb6LVG-v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "question = '李宏毅幾班?'\r\n",
        "paragraph = '李宏毅幾班大金。'\r\n",
        "inputs = chi_tokenizer(question, paragraph, return_tensors='pt')\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    output = model(**inputs)\r\n",
        "# output = model(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\r\n",
        "\r\n",
        "print(\"start_logits: \")\r\n",
        "print(output.start_logits)\r\n",
        "\r\n",
        "print(\"end_logits: \")\r\n",
        "print(output.end_logits)\r\n",
        "\r\n",
        "start = torch.argmax(output.start_logits)\r\n",
        "end = torch.argmax(output.end_logits)\r\n",
        "print(\"start position: \", start.item())\r\n",
        "print(\"end position:   \", end.item())\r\n",
        "\r\n",
        "predict_id = inputs['input_ids'][0][start : end + 1]\r\n",
        "print(\"predict_id:     \", predict_id)\r\n",
        "\r\n",
        "predict_answer = chi_tokenizer.decode(predict_id)\r\n",
        "print(\"predict_answer: \", predict_answer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_logits: \n",
            "tensor([[-0.4426, -1.2055, -0.9898, -0.8487,  0.0632, -0.5500, -0.3370, -0.7374,\n",
            "         -1.0589, -1.1773, -0.8922,  0.0775, -0.1733,  1.7117, -0.5467, -0.2955,\n",
            "         -0.7374]])\n",
            "end_logits: \n",
            "tensor([[-0.7634, -1.4479, -1.3643, -1.3158, -0.7851, -0.3256, -0.4606, -2.0774,\n",
            "         -1.2740, -1.5332, -1.6222, -0.7891, -0.5642,  0.1027,  2.3907, -0.5236,\n",
            "         -2.0774]])\n",
            "start position:  13\n",
            "end position:    14\n",
            "predict_id:      tensor([1920, 7032])\n",
            "predict_answer:  大 金\n"
          ]
        }
      ],
      "metadata": {
        "id": "-sRQGEesjBmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training (Chinese)\n",
        "For Question Answering, loss is the sum of cross entropy between the model prediction and correct answer"
      ],
      "metadata": {
        "id": "fBGQCyCZUwaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "output = model(**inputs, start_positions=torch.tensor([13]), end_positions=torch.tensor([14]))\r\n",
        "print(\"loss: \", output.loss)\r\n",
        "\r\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\r\n",
        "output.loss.backward()\r\n",
        "optimizer.step()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  tensor(2.7629, grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "metadata": {
        "id": "dP3SRdS8wedu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing (English)"
      ],
      "metadata": {
        "id": "awNHFgkCyj9F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "source": [
        "question = \"Why does Jeanie like Tom?\"\r\n",
        "paragraph = \"Jeanie likes Tom because Tom is good at deep learning.\"\r\n",
        "inputs = eng_tokenizer(question, paragraph, return_tensors='pt')\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    output = model(**inputs)\r\n",
        "# output = model(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\r\n",
        "\r\n",
        "print(\"start_logits: \")\r\n",
        "print(output.start_logits)\r\n",
        "\r\n",
        "print(\"end_logits: \")\r\n",
        "print(output.end_logits)\r\n",
        "\r\n",
        "start = torch.argmax(output.start_logits)\r\n",
        "end = torch.argmax(output.end_logits)\r\n",
        "print(\"start position: \", start.item())\r\n",
        "print(\"end position:   \", end.item())\r\n",
        "\r\n",
        "predict_id = inputs['input_ids'][0][start : end + 1]\r\n",
        "print(\"predict_id:     \", predict_id)\r\n",
        "\r\n",
        "predict_answer = eng_tokenizer.decode(predict_id)\r\n",
        "print(\"predict_answer: \", predict_answer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_logits: \n",
            "tensor([[ 0.3904,  0.7138,  0.2942, -0.3142, -0.2484,  0.2123,  0.8086,  0.3086,\n",
            "         -0.2774,  0.0656, -0.0531,  0.1067,  1.4656,  0.5646,  2.3540,  0.6178,\n",
            "          0.1550, -0.2313,  0.2991, -0.4581, -0.4918, -0.2774]])\n",
            "end_logits: \n",
            "tensor([[-1.5364, -1.3534, -1.6117, -2.0081, -1.7822, -1.8132, -1.0893, -1.2300,\n",
            "         -2.0936, -1.7374, -1.5145, -1.4858, -1.4943, -1.4478, -1.1967, -0.8906,\n",
            "         -0.6637, -0.0219,  1.5808,  2.4958, -0.9445, -2.0936]])\n",
            "start position:  14\n",
            "end position:    19\n",
            "predict_id:      tensor([2545, 1110, 1363, 1120, 1996, 3776])\n",
            "predict_answer:  Tom is good at deep learning\n"
          ]
        }
      ],
      "metadata": {
        "id": "gt0Glg0yyfg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training (English)\n",
        "For Question Answering, loss is the sum of cross entropy between the model prediction and correct answer"
      ],
      "metadata": {
        "id": "fQB51L0-yn9k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "output = model(**inputs, start_positions=torch.tensor([14]), end_positions=torch.tensor([19]))\r\n",
        "print(\"loss: \", output.loss)\r\n",
        "\r\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\r\n",
        "output.loss.backward()\r\n",
        "optimizer.step()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  tensor(2.9497, grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "metadata": {
        "id": "82AZ3K7e8ZjS"
      }
    }
  ]
}